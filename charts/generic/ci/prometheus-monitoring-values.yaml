# Example: Application with Prometheus Monitoring
# Open-source monitoring stack with ServiceMonitor, PodMonitor, and PrometheusRule

nameOverride: "prometheus-app"

workload:
  type: deployment
  enabled: true
  replicas: 3

image:
  repository: myorg/metrics-app
  tag: "2.1.0"
  pullPolicy: IfNotPresent

deployment:
  # Expose Prometheus metrics endpoint
  ports:
    - name: http
      containerPort: 8080
      protocol: TCP
    - name: metrics
      containerPort: 9090
      protocol: TCP

  env:
    - name: METRICS_PORT
      value: "9090"
    - name: METRICS_PATH
      value: "/metrics"
    - name: LOG_LEVEL
      value: "info"

  resources:
    limits:
      memory: "512Mi"
      cpu: "500m"
    requests:
      cpu: "100m"
      memory: "256Mi"

  livenessProbe:
    httpGet:
      path: /healthz
      port: http
    initialDelaySeconds: 30
    periodSeconds: 10

  readinessProbe:
    httpGet:
      path: /readyz
      port: http
    initialDelaySeconds: 5
    periodSeconds: 5

service:
  enabled: true
  type: ClusterIP
  ports:
    - name: http
      port: 80
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 9090
      targetPort: metrics
      protocol: TCP

# ServiceMonitor for Prometheus Operator
serviceMonitor:
  enabled: true
  interval: 30s
  scrapeTimeout: 10s
  path: /metrics
  port: metrics

  # Additional labels for Prometheus to discover
  labels:
    prometheus: kube-prometheus
    release: prometheus-operator

  # Relabeling configurations
  relabelings:
    - sourceLabels: [__meta_kubernetes_pod_node_name]
      targetLabel: node
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod

  # Metric relabeling
  metricRelabelings:
    - regex: "go_.*"
      action: drop
    - sourceLabels: [__name__]
      regex: "http_request_duration_seconds.*"
      targetLabel: __tmp_histogram
      replacement: "true"

# PodMonitor for direct pod scraping
podMonitor:
  enabled: true
  interval: 15s
  scrapeTimeout: 5s
  path: /metrics
  port: metrics
  scheme: http

  labels:
    prometheus: kube-prometheus

  # Namespace selector
  namespaceSelector:
    matchNames:
      - default
      - production

  # Sample and label limits
  sampleLimit: 10000
  targetLimit: 10
  labelLimit: 30
  labelNameLengthLimit: 100
  labelValueLengthLimit: 200

  # Custom endpoints for different metrics
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
    - port: metrics
      interval: 60s
      path: /custom-metrics
      params:
        module: [custom]

  # Additional monitors
  additionalMonitors:
    slow-queries:
      interval: 60s
      path: /metrics/slow-queries
      port: metrics
      labels:
        metrics_type: slow_queries

# PrometheusRule for alerting
prometheusRule:
  enabled: true

  labels:
    prometheus: kube-prometheus
    role: alert-rules

  groups:
    - name: prometheus-app.rules
      interval: 30s
      rules:
        # High error rate alert
        - alert: HighErrorRate
          expr: |
            (
              sum(rate(http_requests_total{job="{{ include "generic.fullname" . }}",status=~"5.."}[5m]))
              /
              sum(rate(http_requests_total{job="{{ include "generic.fullname" . }}"}[5m]))
            ) > 0.05
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "High error rate detected for {{ $labels.namespace }}/{{ $labels.pod }}"
            description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes"
            runbook_url: "https://wiki.example.com/runbooks/high-error-rate"

        # High latency alert
        - alert: HighLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket{job="{{ include "generic.fullname" . }}"}[5m])) by (le)
            ) > 0.5
          for: 10m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "High latency detected for {{ $labels.namespace }}/{{ $labels.job }}"
            description: "95th percentile latency is {{ $value }}s for the last 10 minutes"

        # Pod unavailable alert
        - alert: PodUnavailable
          expr: |
            up{job="{{ include "generic.fullname" . }}"} == 0
          for: 5m
          labels:
            severity: critical
            team: platform
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is unavailable"
            description: "Pod has been unavailable for more than 5 minutes"

        # Memory usage alert
        - alert: HighMemoryUsage
          expr: |
            (
              container_memory_working_set_bytes{pod=~"{{ include "generic.fullname" . }}-.*"}
              /
              container_spec_memory_limit_bytes{pod=~"{{ include "generic.fullname" . }}-.*"}
            ) > 0.8
          for: 5m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "High memory usage for {{ $labels.namespace }}/{{ $labels.pod }}"
            description: "Memory usage is {{ $value | humanizePercentage }} of limit"

        # CPU throttling alert
        - alert: CPUThrottlingHigh
          expr: |
            rate(container_cpu_cfs_throttled_seconds_total{pod=~"{{ include "generic.fullname" . }}-.*"}[5m]) > 0.1
          for: 15m
          labels:
            severity: warning
            team: platform
          annotations:
            summary: "CPU throttling detected for {{ $labels.namespace }}/{{ $labels.pod }}"
            description: "CPU has been throttled {{ $value | humanize }}s per second for 15 minutes"

    - name: prometheus-app.slo
      interval: 30s
      rules:
        # SLI: Availability
        - record: slo:service_availability:ratio_rate5m
          expr: |
            sum(rate(http_requests_total{job="{{ include "generic.fullname" . }}",status!~"5.."}[5m]))
            /
            sum(rate(http_requests_total{job="{{ include "generic.fullname" . }}"}[5m]))

        # SLI: Latency
        - record: slo:service_latency_p99:ratio_rate5m
          expr: |
            histogram_quantile(0.99,
              sum(rate(http_request_duration_seconds_bucket{job="{{ include "generic.fullname" . }}"}[5m])) by (le)
            )

        # Multi-window error budget burn rate
        - alert: ErrorBudgetBurn
          expr: |
            (
              slo:service_availability:ratio_rate5m < 0.995
              AND
              slo:service_availability:ratio_rate1h < 0.995
            )
          labels:
            severity: warning
            team: platform
            slo: "99.5% availability"
          annotations:
            summary: "Error budget burn rate is high"
            description: "Service availability is {{ $value | humanizePercentage }}"

  # Additional custom rules
  additionalRules:
    business-metrics:
      - name: business.rules
        rules:
          - record: business:orders_per_minute
            expr: sum(rate(orders_created_total[1m]))
          - record: business:revenue_per_hour
            expr: sum(increase(revenue_total[1h]))

# Autoscaling based on custom metrics
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"
    - type: External
      external:
        metric:
          name: queue_depth
          selector:
            matchLabels:
              queue: '{{ include "generic.fullname" . }}'
        target:
          type: Value
          value: "50"

# Network policy
networkPolicy:
  enabled: true
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow Prometheus scraping
    - from:
        - namespaceSelector:
            matchLabels:
              name: prometheus
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: prometheus
      ports:
        - protocol: TCP
          port: 9090
    # Allow application traffic
    - from:
        - podSelector: {}
      ports:
        - protocol: TCP
          port: 8080
  egress:
    # Allow DNS
    - to:
        - namespaceSelector:
            matchLabels:
              name: kube-system
      ports:
        - protocol: UDP
          port: 53
    # Allow external services
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
      ports:
        - protocol: TCP
          port: 443

# Service account for metrics
serviceAccount:
  enabled: true
  annotations:
    prometheus.io/scrape: "true"

# Pod annotations for Prometheus discovery
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"
  prometheus.io/path: "/metrics"

# Grafana dashboard ConfigMap
extraConfigMaps:
  grafana-dashboard:
    labels:
      grafana_dashboard: "1"
    data:
      prometheus-app-dashboard.json: |
        {
          "dashboard": {
            "title": "Prometheus App Metrics",
            "panels": [
              {
                "title": "Request Rate",
                "targets": [
                  {
                    "expr": "sum(rate(http_requests_total{job=\"{{ include "generic.fullname" . }}\"}[5m])) by (status)"
                  }
                ]
              },
              {
                "title": "P95 Latency",
                "targets": [
                  {
                    "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{job=\"{{ include "generic.fullname" . }}\"}[5m])) by (le))"
                  }
                ]
              },
              {
                "title": "Error Rate",
                "targets": [
                  {
                    "expr": "(sum(rate(http_requests_total{job=\"{{ include "generic.fullname" . }}\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{job=\"{{ include "generic.fullname" . }}\"}[5m]))) * 100"
                  }
                ]
              }
            ]
          }
        }
